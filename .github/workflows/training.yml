name: Model Training
on:
  schedule:
    - cron: '30 * * * *'
  workflow_dispatch:

jobs:
  train-model:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    permissions:
      actions: read
      contents: read

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests xgboost scikit-learn pandas numpy joblib
        pip list
        
    - name: Download features
      run: |
        python <<EOF
import requests
import os
import zipfile

# Get the latest artifact
api_url = f'https://api.github.com/repos/{os.environ["GITHUB_REPOSITORY"]}/actions/artifacts'
headers = {
    'Authorization': f'Bearer {os.environ["GITHUB_TOKEN"]}',
    'Accept': 'application/vnd.github.v3+json'
}

response = requests.get(api_url, headers=headers)
response.raise_for_status()

# Find most recent aqi-features artifact
artifacts = [a for a in response.json()['artifacts'] 
            if a['name'].startswith('aqi-features')]

if not artifacts:
    raise Exception('No matching artifacts found')
    
latest = max(artifacts, key=lambda x: x['updated_at'])
print(f'Downloading artifact: {latest["name"]} (ID: {latest["id"]})')

# Download
os.makedirs('data', exist_ok=True)
download_url = latest['archive_download_url']
response = requests.get(download_url, headers=headers, stream=True)
response.raise_for_status()

zip_path = 'data/features.zip'
with open(zip_path, 'wb') as f:
    for chunk in response.iter_content(chunk_size=8192):
        if chunk:
            f.write(chunk)
        
# Extract
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('data/')
os.remove(zip_path)
print('Download and extraction completed successfully')
EOF
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Verify features
      run: |
        python <<EOF
import pandas as pd
import os

csv_path = 'data/hourly_features.csv'
if not os.path.exists(csv_path):
    print('::error::Features file missing at ' + csv_path)
    print('Directory contents:', os.listdir('data'))
    exit(1)
    
try:
    df = pd.read_csv(csv_path)
    required_columns = ['feature1', 'feature2', 'target']
    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        print(f'::error::Missing required columns: {missing_cols}')
        exit(1)
    print(f'Successfully loaded {len(df)} records')
    print('Columns:', df.columns.tolist())
    print('Sample data:\n', df.head(2))
except Exception as e:
    print(f'::error::Failed to load features: {str(e)}')
    exit(1)
EOF
        
    - name: Train model
      run: |
        mkdir -p models
        python <<EOF
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
import pandas as pd
import joblib
from sklearn.metrics import mean_squared_error

df = pd.read_csv('data/hourly_features.csv')
X = df.drop(columns=['target_column'])
y = df['target_column']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
model.fit(X_train, y_train)

predictions = model.predict(X_test)
mse = mean_squared_error(y_test, predictions)
print(f'Model trained with MSE: {mse:.4f}')

joblib.dump(model, 'models/aqi_model.joblib')
print('Model saved successfully')
EOF
      
    - name: Upload model
      uses: actions/upload-artifact@v4
      with:
        name: aqi-model-${{ github.run_number }}
        path: models/
        retention-days: 7
